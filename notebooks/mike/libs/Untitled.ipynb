{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tweepy.streaming import StreamListener\n",
    "import pprint as pp\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.getenv(\"consumer_key\")\n",
    "consumer_secret = os.getenv(\"consumer_secret\")\n",
    "consumer_token = os.getenv(\"access_token\")\n",
    "access_token_secret = os.getenv(\"access_token_secret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorize_twitter_api():\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    \n",
    "    return auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a6043bd49e23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'auth' is not defined"
     ]
    }
   ],
   "source": [
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8ad20dcac260>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m for tweet in tweepy.Cursor(api.search, q='#contentmarketing', count=20000, \n\u001b[0m\u001b[0;32m      2\u001b[0m     lang='en', since='2017-06-20').items():\n\u001b[0;32m      3\u001b[0m         \u001b[0mtweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "for tweet in tweepy.Cursor(api.search, q='#contentmarketing', count=20000, \n",
    "    lang='en').items():\n",
    "        tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mike whats our count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_api_call(search_term,tweet_mode = 'extended',count = 5, lang='en',  **kwargs):\n",
    "    \"\"\"Makes twitter api call using tweepy\"\"\"\n",
    "    tweets  = api.search(\n",
    "        q = search_term,\n",
    "        tweet_mode = tweet_mode,\n",
    "        count =  count,\n",
    "        lang = lang, \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    )\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sw = set(stopwords.words('english'))  \n",
    "#adding custom stopwords to our stopwords set\n",
    "custom_sw =['char','chars','say']\n",
    "sw.update(custom_sw)\n",
    "regex = re.compile(r\"[^a-zA-Z ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tokens(text):\n",
    "    \"\"\"Cleans and tokenizes text using nltk stopwords, regex to remove symols, lemmatizer.\"\"\"\n",
    "    #Apply regex filter \n",
    "    regex_filter = regex.sub('', text)\n",
    "    #Get Tokens\n",
    "    tokens = word_tokenize(regex_filter)\n",
    "    #Set to lowercase if not in Stop Words list\n",
    "    lowered_sw = [word.lower() for word in tokens if word.lower() not in sw]\n",
    "    #get lemmatized clean tokens\n",
    "    clean_tokens = [lemmatizer.lemmatize(word) for word in lowered_sw]\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_api_df(articles_response,search_term,tokenize=False,sentiment=False,**kwargs):\n",
    "    \n",
    "  \n",
    "\n",
    "        articles_list = []\n",
    "\n",
    "        for article in articles_response['articles']:\n",
    "            try:\n",
    "                articles_list.append({\n",
    "                    \"date\":article['publishedAt'][:10],\n",
    "                    \"source\":article['source']['name'],\n",
    "                    \"url\":article['url'],\n",
    "                    \"author\":article['author'],\n",
    "                    \"text\":article['content'],\n",
    "                    \"search_term\":search_term\n",
    "                })\n",
    "\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        articles_df = pd.DataFrame(articles_list)\n",
    "\n",
    "        columns = ['date','search_term','source','url','author','text']\n",
    "\n",
    "        articles_df = articles_df[columns]\n",
    "\n",
    "        articles_df['text'] = articles_df['text'].astype(str)\n",
    "\n",
    "        if tokenize:\n",
    "\n",
    "            articles_df['clean_tokens'] = articles_df['text'].apply(clean_tokens)\n",
    "\n",
    "            if sentiment:\n",
    "\n",
    "                sentiment_score_list = []\n",
    "\n",
    "                for article in articles_df['clean_tokens']:\n",
    "                    string = \" \".join(article)\n",
    "                    sentiment_score = analyzer.polarity_scores(string)\n",
    "                    sentiment_score_list.append(sentiment_score)\n",
    "\n",
    "                sentiment_df = pd.DataFrame(sentiment_score_list)\n",
    "\n",
    "                sentiment_df.columns = ['compound','negative','neutral','positive']\n",
    "\n",
    "                articles_df = pd.concat([articles_df, sentiment_df], axis=1)\n",
    "\n",
    "                return articles_df\n",
    "\n",
    "            else:\n",
    "\n",
    "                return articles_df\n",
    "\n",
    "        if sentiment:\n",
    "\n",
    "            sentiment_score_list = []\n",
    "\n",
    "            for article in articles_df['text']:\n",
    "                sentiment_score = analyzer.polarity_scores(article)\n",
    "                sentiment_score_list.append(sentiment_score)\n",
    "\n",
    "            sentiment_df = pd.DataFrame(sentiment_score_list)\n",
    "\n",
    "            sentiment_df.columns = ['compound','negative','neutral','positive']\n",
    "\n",
    "            articles_df = pd.concat([articles_df, sentiment_df], axis=1)\n",
    "\n",
    "            return articles_df\n",
    "\n",
    "        else:\n",
    "\n",
    "            return articles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_df(search_term,**kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper for news_api_call, clean_tokens, and news_api_df\n",
    "    most common syntax is df = sentiment_df(\"search_term\",tokenize=True,sentiment=True)\n",
    "    Optional Parameters for api call: language(default=\"en\"), page_size(default=100)\n",
    "    sort_by(default=\"relevancy\"),sources(default=None),domains(default=None)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    return news_api_df(news_api_call(search_term,**kwargs),search_term,**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
